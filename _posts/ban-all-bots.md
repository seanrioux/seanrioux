---
layout: post
title: "How To Fix The Web: Ban The Bots"
category: web
author: sean
summary: Social media is inundated with bots engaged in platform manipulation. These bots are programmed to amplify harmful discourse, sow polarization, and as we have seen already this year, lead to real world chaos.
---

Social media is inundated with bots engaged in platform manipulation. These bots are programmed to amplify harmful discourse, sow polarization, and as we have seen already this year, lead to real world chaos.

Despite violent attacks on democracy social media platforms continue to manage the problem reactively, if not retroactively. Mass culling of accounts after real world harm occurs is not a sustainable solution. Platforms need to begin to act proactively, restrictively, and on multiple fronts to battle manipulation.

This means a change to their incentives, and their business fundamentals, and it will cost them. In the meantime, as misinformation continues to draw out this global pandemic, as blood is shed on the steps of the US capital building, what are their continued half measures costing us?

NOTE: Artificial content

### Scale

The scale of this issue is global. Some humbling numbers:

- A 2017 study estimated that 9%-15% of Twitter accounts were bots
- A 2019 CNN report found Facebook removed 5.4 billion fake accounts
- [Nearly half of accounts Tweeting about Coronavirus are likely bots](https://www.npr.org/sections/coronavirus-live-updates/2020/05/20/859814085/researchers-nearly-half-of-accounts-tweeting-about-coronavirus-are-likely-bots)

### Impact

- [The next-generation bots interfering with the US election](https://www.nature.com/articles/d41586-020-03034-5)
- [Researchers: Nearly Half Of Accounts Tweeting About Coronavirus Are Likely Bots](https://www.npr.org/sections/coronavirus-live-updates/2020/05/20/859814085/researchers-nearly-half-of-accounts-tweeting-about-coronavirus-are-likely-bots)
- [Non-human users threaten to hijack Canada's Twitter discussions](https://www.nationalobserver.com/2019/06/26/news/non-human-users-threaten-hijack-canadas-twitter-discussions)

## Causes

When we consider platform mechanics and business incentives we find many possible factors:

### Prioritizing user volume

From the beginning social platforms have prioritized user volume. As publicly traded companies, with revenue tied to advertising, pressure on upward user growth is primary. This pursuit of user led-growth has lead to a general industry goal of low friction sign-up, so low, that the process can be easily automated for most services.

Furthermore, it is possible correlating user growth and revenue acts as a perverse incentive; leading companies to ignore fake users, in the interest of inflated growth metrics.

### Lack of user verification

In pursuit of low friction sign-up, these services have resisted more effective user verification measures. While many services has recently begun to encourage two factor authentication or offer voluntary verification programs, for the general user no more than an email is a required to sign-up for most.

This is problematic. Email is not as a reliable indicator of authentic activity as an email server can be hosted anywhere, and accounts created on mass via automation.

### Interaction via API

As human users of Twitter we use an app or the web interface. An API, (or application programming interface) allows programmers to build software automation to interact with the service, also known. Depending on the service, these bots can do close to everything that is possible through the app or web interface, but without the limitations of the user interface or a human user: bots can work faster, more efficiently, and around the clock.

Why would they allow this you might ask? The purpose of a social API is to allow third-party businesses to leverage the platforms to build new products and services. If you use Buffer or Hoot-suite, for example, you're leveraging APIs.

While perhaps beneficial in these applications, these same APIs may also be exploited by bad actors. Furthermore, sorting legitimate and illegitimate use is a non-trivial task, inevitably requiring human moderation for millions, if not billions of individual automated interactions.

Most problematically, perhaps, user are typically not even aware this automation is possible or prevalent, meaning legitimate or not, many of us are interacting with automated content opaquely (i.e., we have no indication that we are interacting with an API driven automation).

## Possible platform solutions

As Jack Dorsey was apt to point out in a recent interview on the New York Times Daily podcast, there is no magic bullet to solving the issues of social media and changes must be carefully considered. For the most part the fundamental mechanisms of these platforms were not designed with a great deal of consideration for their broad social impact. Learning from that mistake, it is essential as we consider any change to these platforms carefully to mitigate future negative impact.

This, however, cannot be used as an excuse for inaction, or more importantly deep changes. This is where it is important for these companies to have the courage to challenges deeply held assumptions inherent to the their businesses, and the web. Things like the inherent anonymity of the web, and business models that rely on quantity over quality must be challenged, considered and discussed openly.

### Verify every account

Starting with new registrations and working through all accounts, all social media accounts should be verified, and determined to be held by a specific individual. While services might opt to allow unverified accounts, these accounts should be flagged as unverified (similar to the current Twitter verified badge) to ensure humans are clearly able to delineate between trusted human sources, and possible non-human actors on the platform.

There are a few methods that could be used here (some already in practice for access to certain features) either in tandem or as part of a suite of possible options:

- **SMS based verification:** a user receives an SMS containing a unique code on registration, which they use, in conjunction with their password, to access the service and on every subsequent sign-in (i.e., two factor authentication).
- **Government issued ID verification:** a user submits a photo of a valid government issued ID which is checked for authenticity and uniqueness.
- **Human profile photo verification:** a user must maintain at least one well lit, machine and human identifiable image of their face (ideally matching that of the ID).
- **Peer verification:** a new user may request verification from peers who are verified on the service to confirm their identity.
- **Address verification:** a new user must submit a mailing address to which they have access. The user receives a postcard from the service to that address, which includes return postage. The user signs the postcard, and mails it back confirming their address.
- **Payment with a credit card:** perhaps the least popular, but one of the most effective and trusted methods would be simply to require users to pay, a one time fee, to process a verification (with a credit card in their name).

To some these verifications might seem draconian (though most, if not all are already a part of official verification policies). To some they might present privacy concerns. What about users seeking to hide their identities (or example, whistle-blowers or other anonymous actors)? To that I would argue, that without a method to properly verify these individuals are who they say they are, that anonymity does more harm than good on these type of platforms.

The traditional news media has long had policies and methods for verifying anonymous sources as trusted, privately, in ways that protect the source but allow them to publish information they assert as truthful. Self published information from anonymous sources is unverifiable, and thus objectively, untrustworthy.

One account per individual If we are to require verification for each account, the natural conclusion is that only one personal account may be held by each human individual. The operative here is personal account, namely that each human user on these services should be presented as such, and verified as such to be trustworthy.

There is, of course, a legitimate use case for engaging via multiple accounts on social media, for example engaging as a business, organization, or a creator. In these cases additional verification measure can be employed. Should the entity hold special designation (for example, they are a registered LLC or hold a trademark) that this special designation can be submitted for verification, and be identified (e.g., as a badge) in the account profile to build trust.

In cases where this is not so, for example when an account is simply for a blog, or other creator without any sort of incorporation of registration of business then the restriction is simple: these accounts can still be maintained, but will lack badges of verification. Once these creators become established and officially designated in some verifiable way they can then apply for verification and improve their standing.

The goal should be that verification, or special badges of designation should indicate trust not truth per se, but traceability back to human actors. In this capacity badges of verification should be prominent, displayed on every post, every profile, every interface.

### Restrict activity for unverified accounts

As not all accounts will chose to verify (and of course, some may be unable to) activity for these accounts should be restricted. This should act as both an incentive for verification, to encourage verification, but also, to marginalize possible non-human actors who are unable to verify.

A number of possible restrictions might have effect:

- **Restrict frequency of posting:** reduce the rate at which unverified users are able to post to the site (e.g., by the minute or even by the hour).
- **Restrict or demote unverified comments:** reduce the prominence or visibility of comments (or other interactions) made by unverified accounts. If you want your voice to be heard, you need a face.
- **Restrict what they can see:** most effectively perhaps (if not sure to be most controversial), limit the amount of content a user can access if unverified. Restrict them to only verified publishers, limit their feed, whatever it takes to create value incentives to verify.

One might make the point, that many social users (if not most on Twitter and Instagram), are passive users who would not inclined to verify. These users might simply leave or accept a restricted experience, reducing their usage. This could amount to a massive loss of revenue to these platforms, as ad revenue is paired to audience size, and the time people spend on these platforms (both toxic key metrics we'll explore in a future post).

But, as an ad buyer, I can't help but wonder, how much of my ad spend goes to non-human actors ignore my ads? If there is not more value to a verified audience?

Fundamentally it's a question quality versus quantity. These platforms may have been built on quantity, but if quantity is artificial its value is lost both to us as users and to advertisers. Refocusing to be providers of greater quality (or at least more verifiably human) engage is a value incentive on both fronts, even if it changes the business metrics.

And that's the key point isn't it? Incentives. Restrictions may be perceived as negative incentives, but if there is no or insufficient incentive to overcome a barrier to entry (like being verified), then this likely reflects a flimsy value proposition. "Easy sign-up" is not a point of value, it's a user experience optimization, and so what are humans getting when we give in to these platforms?

So as we consider restrictions, we must also consider how these platforms can increase the value on offer. Do so, and users will choose to look past restrictions, and verify.

### Flag artificial interactions

The social media platforms have recently begun flagging content as potentially harmful, or misleading, including both alt-right and COVID-19 disinformation. This is a step in the right direction, however it only addresses content that has been reported by users (or determined to be misleading by internal moderators). In these cases a manual intervention is required, and this simply doesn't scale to the issue of artificial content.

At scale, content must be flag proactively for the possibility that there is no human actor behind it. Similar to flags for harmful or misleading information, these flags or alerts would appear before a user is able to interact with the suspected artificial content. This additional friction or indicator would allow users the opportunity to make a value judgement about the content before interacting.

These types of flags could be applied in the following scenarios:

- **Content posted via API:** when an automation interacts with an API resulting in public facing content, for whatever purpose, this should be transparent to the end user. A indication of what specific software may have been used, as well as a warning indicating that the account may or may not be managed by an actual person could help users moderate their desired level of interaction in a more informed way.
- **Content which is highly duplicitous to other content:** while the specifics of algorithms used to determine content as artificial are much to complicated to discuss in the context of this article, there are some immediately obvious indicators, which anyone who spends enough time on social media are aware. Duplicative content for example. Bots will often have a set of templates used to create posts. While these could be sophisticated enough to rotate or rearrange words to create original sounding content, it's often the case that the content is still highly duplicated across the platforms. Even in cases where duplicate content may not be artificial but just simply reflective of a shared perspective, a flag indicating content as high duplicated would encourage greater originality and consideration before reposting.
- **Content which has been flagged, but not confirmed as artificial:**

While certainly some legitimate uses would be caught up in this kind of flagging, in both the cases of API posted content, and duplicitous or artificial seeming content, a virtuous cycle of prioritized human interaction, authenticity, and originality is perhaps the only true side effect. Any argument that supposes that somehow the echo chamber of shared outrage is a positive feature of these platforms is delusional.

Yes, certainly collective outrage associated with a movement like Black Lives Matter, or the Arab Spring may result in positive outcomes in society; a freer more fair and democratic world is the promise, but at times also disproportionate reaction and violence. 



both when it's clear to them that the content has been posted in an automated way and when unverified users are posted content suspected to be

## Empower user moderation

- Create a moderator team:
- Respond more actively via flagging:

## What's coming

No-one is prepared for how disruptive artificial intelligence is to become in the coming decade. and in this case social media companies have done little to moderate their possible use in developing next generation bots.

At the moment, the non-human actors we're considering here are simple automated bots, designed to mimic human interaction.

However, if we look at the field of machine learning, and the ease at which deep fakes can produce fake human profile images, video, audio, or how bots can be trained to create original and convincing written text, and the problem of these non-human actors owning may become intractable.

This is why I'm recommending a few premptive actions:

- Ban AI generated content

This it's why companies act now: to work to ban any and all automated activity, and to allow human communications to be human scale.
