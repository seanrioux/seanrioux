<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://example.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://example.com/" rel="alternate" type="text/html" /><updated>2021-01-12T22:14:56-05:00</updated><id>https://example.com/feed.xml</id><title type="html">Sean Rioux</title><subtitle>This is it. This is a blog template and it's tight. Seriously, it's got everything it needs. Got it. Good.</subtitle><entry><title type="html">How To Fix The Web: Ban All Bots</title><link href="https://example.com/article/2019-02-04-welcome-to-jekyll" rel="alternate" type="text/html" title="How To Fix The Web: Ban All Bots" /><published>2019-02-04T00:00:00-05:00</published><updated>2019-02-04T00:00:00-05:00</updated><id>https://example.com/article/welcome-to-jekyll</id><content type="html" xml:base="https://example.com/article/2019-02-04-welcome-to-jekyll">&lt;p&gt;Social networks are inundated with bots and fake accounts. It is estimated that
9%-15% of the roughly 340 million twitter accounts may not be human (comparable
to the population of Canada in terms of total “individuals”). For Facebook the
estimated percentage of fake accounts is lower, but a significant volume
nonetheless, with an estimated 5% of their 2.5 billion users fake (or 125
million fake accounts).&lt;/p&gt;

&lt;p&gt;A study conducted in the wake of the 2016 US election found that 19% of all
Tweets during the period leading up to the election we’re posted by 400,000
bots. The US Senate Intelligence Committee, as part of the Russia Report found a
concerning amount of bot activity to be part of state sanctioned attacks against
the US, by Russia.&lt;/p&gt;

&lt;p&gt;And it isn’t limited to the US. In Canada, similar analysis has lead to similar
conclusions. While not always state sanctioned, bad actors are actively
leveraging non-human activity to manipulate conversation at a global scale.&lt;/p&gt;

&lt;p&gt;While efforts by social media companies to combat the impact of non-human
activities on their platforms have been fruitful, the problem persists, and it’s
getting worse.&lt;/p&gt;

&lt;h2 id=&quot;root-causes&quot;&gt;Root Causes&lt;/h2&gt;

&lt;p&gt;Why have these platforms allowed such a persistent and identifiable problem to
reach this level of severity?&lt;/p&gt;

&lt;h3 id=&quot;prioritizing-user-volume&quot;&gt;Prioritizing user volume&lt;/h3&gt;

&lt;p&gt;From the beginning these services have prioritized user volume as a key metric.
As publicly traded companies, with revenue tied to advertising, user growth has
been a proxy for revenue growth and general platform health. The pursuit of user
volume over time has lead to a general goal of low friction sign-up, so low,
that the process can be automated ad infinitum.&lt;/p&gt;

&lt;h3 id=&quot;lack-of-user-verification&quot;&gt;Lack of user verification&lt;/h3&gt;

&lt;p&gt;In pursuit of low friction sign-up, these services have thus far resisted more
effective user verification measures. While Twitter has recently begun to employ
phone verification in cases of suspicious activity, and both Twitter and
Facebook offering voluntary verification programs, for the general user no more
than an email is a required.&lt;/p&gt;

&lt;p&gt;This is fundamentally problematic as email can not act as a reliable indicator
of authentic human activity. An email server can easily be self hosted, and
email accounts easily created on mass.&lt;/p&gt;

&lt;p&gt;While certainly at 340 million users on Twitter and 2.5 billion users on
Facebook, verification of all accounts presents a non-trivial technical
challenge. However, that email continues to be all that is required for sign-up
for platforms of this scale and impact, signals a general lack of effort toward
true human-user verification in favour of maintaining user volume and growth.&lt;/p&gt;

&lt;h3 id=&quot;third-party-interaction-via-api&quot;&gt;Third party interaction via API&lt;/h3&gt;

&lt;p&gt;In the early days of these services part of the promise (of Twitter in
particular) was that of a more open web. Twitter would become the defacto mass
messaging service with an easy to use API (application programming interface) to
allow all kinds of third-party services to leverage the platform to bring Tweets
to other services, and other services to Twitter.&lt;/p&gt;

&lt;p&gt;If you’re not familiar with the technology, an API allows programmers to
interact with software, like Twitter, programmatically. As human users of
Twitter we typically use an interface (twitter.com or a mobile app). This
interface is an abstraction layer on top of the API that makes it easy for
non-programmers to interact with information. The API, conversely allows
programmers to interact directly with the information. They can get information,
or post information to the service using this API, close to everything that is
possible through the interface, but without the limitations of the user
interface.&lt;/p&gt;

&lt;p&gt;Many business models have been built atop this API based model. If you use
Buffer or Hootsuite as part of your digital marketing activities, you’re
leveraging APIs.&lt;/p&gt;

&lt;p&gt;While powerful, and beneficial in some applications, for example automating
brand messaging across marketing channels or simply creating interactive or
informational programs, these avenues are now broadly exploited by bad actors.
Sorting out legitimate use and illegitimate use is something these services are
actively engaging, but again the problems persist. At the risk of dissuading
legitimate use these services have yet to fully reign in misuse of their APIs,
and with few users even aware this automation is possible many of us are
interacting with this misuse opaquely.&lt;/p&gt;

&lt;h3 id=&quot;lack-of-regulating-ai&quot;&gt;Lack of regulating AI&lt;/h3&gt;

&lt;p&gt;No-one is prepared for how disruptive artificial intelligence is to become in
the coming decade and beyond, and yet policy makers and technology companies
have done little to nothing to prevent their misuse, and we’re already seeing
the effect on these platforms.&lt;/p&gt;

&lt;p&gt;For the most part the non-human actors on social services we’re considering are
simple automated bots, designed to mimic human interaction with these services,
or simply use the APIs provided by these services as intended.&lt;/p&gt;

&lt;p&gt;However, pair this with ease at which deep fakes can produce fake human profile
images, video, audio, or how bots can be trained to create original and
convincing written text, and the problem of these non-human actors owning these
platforms may become intractable. Most solutions meant to mitigate the
prevalence of bots, attempt to do so by looking for signature bot activity:
monitoring frequency of posting, content originality, or even the nuanced
cadence at which step by step interactions occur. Machine learning algorithms
trained to mimic human behaviour accurately is not a remote possibility, it’s a
present reality, and so it may become impossible to stop non-human actors from
dominating these platforms if we don’t move aggressively to ban them from these
platforms.&lt;/p&gt;

&lt;h2 id=&quot;proposed-platform-solutions&quot;&gt;Proposed platform solutions&lt;/h2&gt;

&lt;p&gt;As Jack Dorsey was apt to point out in a recent interview on the New York Times
Daily podcast, there is no magic bullet to solving the issues of social media
and changes must be carefully considered. For the most part the fundamental
mechanisms of these platforms were not designed with a great deal of
consideration for their broad social impact. Learning from that mistake, it is
essential as we consider any change to these platforms carefully to mitigate
future negative impact.&lt;/p&gt;

&lt;p&gt;This, however, cannot be used as an excuse for inaction, or more importantly
deep changes. This is where it is important for these companies to have the
courage to challenges deeply held assumptions inherent to the their businesses,
and the web. Things like the inherent anonymity of the web, and business models
that rely on quantity over quality must be challenged, considered and discussed
openly.&lt;/p&gt;

&lt;h3 id=&quot;verify-every-account&quot;&gt;Verify every account&lt;/h3&gt;

&lt;p&gt;Starting with new registrations and working through all accounts, all social
media accounts should be verified, and determined to be held by a specific
individual. While services might opt to allow unverified accounts, these
accounts should be flagged as unverified (similar to the current Twitter
verified badge) to ensure humans are clearly able to delineate between trusted
human sources, and possible non-human actors on the platform.&lt;/p&gt;

&lt;p&gt;There are a few methods that could be used here (some already in practice for
access to certain features) either in tandem or as part of a suite of possible
options:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;SMS based verification:&lt;/strong&gt; a user receives an SMS containing a unique code on
registration, which they use, in conjunction with their password, to access
the service and on every subsequent sign-in (i.e., two factor authentication).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Government issued ID verification:&lt;/strong&gt; a user submits a photo of a valid
government issued ID which is checked for authenticity and uniqueness.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Human profile photo verification:&lt;/strong&gt; a user must maintain at least one well
lit, machine and human identifiable image of their face (ideally matching that
of the ID).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Peer verification:&lt;/strong&gt; a new user may request verification from peers who are
verified on the service to confirm their identity.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Address verification:&lt;/strong&gt; a new user must submit a mailing address to which
they have access. The user receives a postcard from the service to that
address, which includes return postage. The user signs the postcard, and mails
it back confirming their address.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Payment with a credit card:&lt;/strong&gt; perhaps the least popular, but one of the most
effective and trusted methods would be simply to require users to pay, a one
time fee, to process a verification (with a credit card in their name). To
some these verifications might seem draconian (though most, if not all are
already a part of official verification policies). To some they might present
privacy concerns. What about users seeking to hide their identities (or
example, whistleblowers or other anonymous actors)? To that I would argue,
that without a method to properly verify these individuals are who they say
they are, that anonymity does more harm than good on these type of platforms.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The traditional news media has long had policies and methods for verifying
anonymous sources as trusted, privately, in ways that protect the source but
allow them to publish information they assert as truthful. Self published
information from anonymous sources is unverifiable, and thus objectively,
untrustworthy.&lt;/p&gt;

&lt;p&gt;One account per individual If we are to require verification for each account,
the natural conclusion is that only one personal account may be held by each
human individual. The operative here is personal account, namely that each human
user on these services should be presented as such, and verified as such to be
trustworthy.&lt;/p&gt;

&lt;p&gt;There is, of course, a legitimate use case for engaging via multiple accounts on
social media, for example engaging as a business, organization, or a creator. In
these cases additional verification measure can be employed. Should the entity
hold special designation (for example, they are a registered LLC or hold a
trademark) that this special designation can be submitted for verification, and
be identified (e.g., as a badge) in the account profile to build trust.&lt;/p&gt;

&lt;p&gt;In cases where this is not so, for example when an account is simply for a blog,
or other creator without any sort of incorporation of registration of business
then the restriction is simple: these accounts can still be maintained, but will
lack badges of verification. Once these creators become established and
officially designated in some verifiable way they can then apply for
verification and improve their standing.&lt;/p&gt;

&lt;p&gt;The goal should be that verification, or special badges of designation should
indicate trust not truth per se, but traceability back to human actors. In this
capacity badges of verification should be prominent, displayed on every post,
every profile, every interface.&lt;/p&gt;

&lt;h2 id=&quot;restrict-activity-for-unverified-accounts&quot;&gt;Restrict activity for unverified accounts&lt;/h2&gt;

&lt;p&gt;As not all accounts will chose to verify (and of course, some may be unable to)
activity for these accounts should be restricted. This should act as both an
incentive for verification, to encourage verification, but also, to marginalize
possible non-human actors who are unable to verify.&lt;/p&gt;

&lt;p&gt;A number of possible restrictions might have effect:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Restrict frequency of posting:&lt;/strong&gt; reduce the rate at which unverified users
are able to post to the site (e.g., by the minute or even by the hour).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Restrict or demote unverified comments:&lt;/strong&gt; reduce the prominence or
visibility of comments (or other interactions) made by unverified accounts. If
you want your voice to be heard, you need a face.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Restrict what they can see:&lt;/strong&gt; most effectively perhaps (if not sure to be
most controversial), limit the amount of content a user can access if
unverified. Restrict them to only verified publishers, limit their feed,
whatever it takes to create value incentives to verify.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One might make the point, that many social users (if not most on Twitter and
Instagram), are passive users who would not inclined to verify. These users
might simply leave or accept a restricted experience, reducing their usage. This
could amount to a massive loss of revenue to these platforms, as ad revenue is
paired to audience size, and the time people spend on these platforms (both
toxic key metrics we’ll explore in a future post).&lt;/p&gt;

&lt;p&gt;But, as an ad buyer, I can’t help but wonder, how much of my ad spend goes to
non-human actors ignore my ads? If there not more value to a verified audience?&lt;/p&gt;

&lt;p&gt;Fundamentally it’s a question quality versus quantity. These platforms may have
been built on quantity, but if quantity is artificial it’s value is lost both to
us as users and to advertisers. Refocusing to be providers of greater quality
(or at least more verifiably human) engage is a value incentive on both fronts,
even if it changes the business metrics.&lt;/p&gt;

&lt;p&gt;And that’s the key point isn’t it? Incentives. Restrictions may be perceived as
negative incentives, but if there is no or insufficient incentive to overcome a
barrier to entry (like being verified), then this likely reflects a flimsy value
proposition. “Easy sign-up” is not a point of value, it’s a user experience
optimization, and so what are humans getting when we give in to these platforms?&lt;/p&gt;

&lt;p&gt;So as we consider restrictions, we must also consider how these platforms can
increase the value on offer. Do so, and users will choose to look past
restrictions, and verify.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Flag content posted via API&lt;/li&gt;
  &lt;li&gt;Ban AI generated content&lt;/li&gt;
  &lt;li&gt;Flag suspected AI generated content&lt;/li&gt;
  &lt;li&gt;Empower user moderators&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Direct action&lt;/p&gt;

&lt;p&gt;As&lt;/p&gt;

&lt;p&gt;As brands&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Less automation more human engagement&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As individuals&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Act as moderator contributors&lt;/li&gt;
  &lt;li&gt;Media literacy “how to spot a fake account”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As governments&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Investment in digital security expertise&lt;/li&gt;
  &lt;li&gt;Take the threat seriously&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Sean Rioux</name></author><category term="web" /><summary type="html">Social networks are inundated with bots and fake accounts. It is estimated that 9%-15% of the roughly 340 million twitter accounts may not be human (comparable to the population of Canada in terms of total “individuals”). For Facebook the estimated percentage of fake accounts is lower, but a significant volume nonetheless, with an estimated 5% of their 2.5 billion users fake (or 125 million fake accounts).</summary></entry></feed>